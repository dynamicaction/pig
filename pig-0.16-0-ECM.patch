diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
index 595e68c..a91e4d7 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
@@ -53,6 +53,7 @@ import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
 import org.apache.pig.backend.hadoop.executionengine.JobCreationException;
 import org.apache.pig.backend.hadoop.executionengine.Launcher;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.LastInputStreamingOptimizer;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.CombinerPlanRemover;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.DotMRPrinter;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.EndOfAllInputSetter;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MRIntermediateDataVisitor;
@@ -638,10 +639,18 @@ public class MapReduceLauncher extends Launcher{
                 pc.getProperties().getProperty(
                         "last.input.chunksize", JoinPackager.DEFAULT_CHUNK_SIZE);
 
-        String prop = pc.getProperties().getProperty(PigConfiguration.PIG_EXEC_NO_COMBINER);
-        if (!pc.inIllustrator && !("true".equals(prop)))  {
-            boolean doMapAgg =
-                    Boolean.valueOf(pc.getProperties().getProperty(PigConfiguration.PIG_EXEC_MAP_PARTAGG,"false"));
+        boolean doMapAgg =
+            Boolean.valueOf(pc.getProperties().getProperty(PigConfiguration.PROP_EXEC_MAP_PARTAGG,"false"));
+
+        boolean doCombiner =
+            !Boolean.valueOf(pc.getProperties().getProperty(PigConfiguration.PROP_NO_COMBINER,
+                    // Default to no combiner when doMapAgg is true
+                    String.valueOf(doMapAgg)));
+
+        // If doMapAgg is true, add the combiner even if it was disabled, since
+        // CombinerOptimizer actually adds the POPartialAgg plan. We'll remove
+        // the combine plan later.
+        if (!pc.inIllustrator && (doCombiner || doMapAgg))  {
             CombinerOptimizer co = new CombinerOptimizer(plan, doMapAgg);
             co.visit();
             //display the warning message(s) from the CombinerOptimizer
@@ -660,7 +669,7 @@ public class MapReduceLauncher extends Launcher{
             la.adjust();
         }
         // Optimize to use secondary sort key if possible
-        prop = pc.getProperties().getProperty(PigConfiguration.PIG_EXEC_NO_SECONDARY_KEY);
+        String prop = pc.getProperties().getProperty(PigConfiguration.PIG_EXEC_NO_SECONDARY_KEY);
         if (!pc.inIllustrator && !("true".equals(prop)))  {
             SecondaryKeyOptimizerMR skOptimizer = new SecondaryKeyOptimizerMR(plan);
             skOptimizer.visit();
@@ -670,6 +679,13 @@ public class MapReduceLauncher extends Launcher{
         POPackageAnnotator pkgAnnotator = new POPackageAnnotator(plan);
         pkgAnnotator.visit();
 
+        // now we can remove the combiner plan if we don't really need it.
+        // note the combiner plan is needed by POPackageAnnotator
+        if (!doCombiner){
+            CombinerPlanRemover remover = new CombinerPlanRemover(plan);
+            remover.visit();
+        }
+
         // optimize joins
         LastInputStreamingOptimizer liso =
                 new MRCompiler.LastInputStreamingOptimizer(plan, lastInputChunkSize);
@@ -823,4 +839,3 @@ public class MapReduceLauncher extends Launcher{
         }
     }
 }
-
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MultiQueryOptimizer.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MultiQueryOptimizer.java
index 58e9da4..57b2196 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MultiQueryOptimizer.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MultiQueryOptimizer.java
@@ -1204,6 +1204,7 @@ class MultiQueryOptimizer extends MROpPlanVisitor {
         MultiQueryPackager pkgr = new MultiQueryPackager();
         pkgr.setInCombiner(inCombiner);
         pkgr.setSameMapKeyType(sameMapKeyType);
+        pkgr.setNumInputs(pkg.getNumInps());
         pkg.setPkgr(pkgr);
         return pkg;
     }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPartialAgg.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPartialAgg.java
index 1032af1..4440b19 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPartialAgg.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPartialAgg.java
@@ -234,7 +234,7 @@ public class POPartialAgg extends PhysicalOperator implements Spillable, Groupin
                     if (spillProcessedMap || shouldSpill()) {
                         startSpill(false);
                     } else {
-                        LOG.info("Avoided emitting records during spill memory call.");
+                        LOG.debug("Avoided emitting records during spill memory call.");
                         doContingentSpill = false;
                     }
                 }
@@ -266,7 +266,7 @@ public class POPartialAgg extends PhysicalOperator implements Spillable, Groupin
                     if (parentPlan.endOfAllInput) {
                         // parent input is over. flush what we have.
                         inputsExhausted = true;
-                        LOG.info("Spilling last bits.");
+                        LOG.debug("Spilling last bits.");
                         startSpill(true);
                         continue;
                     } else {
@@ -309,7 +309,7 @@ public class POPartialAgg extends PhysicalOperator implements Spillable, Groupin
 
     private void estimateMemThresholds() {
         if (!mapAggDisabled()) {
-            LOG.info("Getting mem limits; considering " + ALL_POPARTS.size()
+            LOG.debug("Getting mem limits; considering " + ALL_POPARTS.size()
                     + " POPArtialAgg objects." + " with memory percentage "
                     + percentUsage);
             MemoryLimits memLimits = new MemoryLimits(ALL_POPARTS.size(), percentUsage);
@@ -325,10 +325,10 @@ public class POPartialAgg extends PhysicalOperator implements Spillable, Groupin
             }
             avgTupleSize = estTotalMem / estTuples;
             long totalTuples = memLimits.getCacheLimit();
-            LOG.info("Estimated total tuples to buffer, based on " + estTuples + " tuples that took up " + estTotalMem + " bytes: " + totalTuples);
+            LOG.debug("Estimated total tuples to buffer, based on " + estTuples + " tuples that took up " + estTotalMem + " bytes: " + totalTuples);
             firstTierThreshold = (int) (0.5 + totalTuples * (1f - (1f / sizeReduction)));
             secondTierThreshold = (int) (0.5 + totalTuples *  (1f / sizeReduction));
-            LOG.info("Setting thresholds. Primary: " + firstTierThreshold + ". Secondary: " + secondTierThreshold);
+            LOG.debug("Setting thresholds. Primary: " + firstTierThreshold + ". Secondary: " + secondTierThreshold);
             // The second tier should at least allow one tuple before it tries to aggregate.
             // This code retains the total number of tuples in the buffer while guaranteeing
             // the second tier has at least one tuple.
@@ -348,12 +348,12 @@ public class POPartialAgg extends PhysicalOperator implements Spillable, Groupin
             int numBeforeReduction = numRecsInProcessedMap + numRecsInRawMap;
             aggregateBothLevels(false, false);
             int numAfterReduction = numRecsInProcessedMap + numRecsInRawMap;
-            LOG.info("After reduction, processed map: " + numRecsInProcessedMap + "; raw map: " + numRecsInRawMap);
-            LOG.info("Observed reduction factor: from " + numBeforeReduction +
+            LOG.debug("After reduction, processed map: " + numRecsInProcessedMap + "; raw map: " + numRecsInRawMap);
+            LOG.debug("Observed reduction factor: from " + numBeforeReduction +
                     " to " + numAfterReduction +
                     " => " + numBeforeReduction / numAfterReduction + ".");
             if ( numBeforeReduction / numAfterReduction < minOutputReduction) {
-                LOG.info("Disabling in-memory aggregation, since observed reduction is less than " + minOutputReduction);
+                LOG.debug("Disabling in-memory aggregation, since observed reduction is less than " + minOutputReduction);
                 disableMapAgg();
             }
             sizeReduction = numBeforeReduction / numAfterReduction;
@@ -436,7 +436,7 @@ public class POPartialAgg extends PhysicalOperator implements Spillable, Groupin
         // If spillingIterator is null, we are already spilling and don't need to set up.
         if (spillingIterator != null) return;
 
-        LOG.info("Starting spill.");
+        LOG.debug("Starting spill.");
         if (aggregate) {
             aggregateBothLevels(false, false);
         }
@@ -448,7 +448,7 @@ public class POPartialAgg extends PhysicalOperator implements Spillable, Groupin
         // if no more to spill, return EOP_RESULT.
         if (processedInputMap.isEmpty()) {
             spillingIterator = null;
-            LOG.info("In spillResults(), processed map is empty -- done spilling.");
+            LOG.debug("In spillResults(), processed map is empty -- done spilling.");
             return EOP_RESULT;
         } else {
             Map.Entry<Object, List<Tuple>> entry = spillingIterator.next();
@@ -533,7 +533,7 @@ public class POPartialAgg extends PhysicalOperator implements Spillable, Groupin
         int processedTuples = numRecsInProcessedMap;
         numRecsInProcessedMap = aggregate(rawInputMap, processedInputMap, numRecsInProcessedMap);
         numRecsInRawMap = 0;
-        LOG.info("Aggregated " + rawTuples+ " raw tuples."
+        LOG.debug("Aggregated " + rawTuples+ " raw tuples."
                 + " Processed tuples before aggregation = " + processedTuples
                 + ", after aggregation = " + numRecsInProcessedMap);
     }
@@ -544,7 +544,7 @@ public class POPartialAgg extends PhysicalOperator implements Spillable, Groupin
         }
         int processedTuples = numRecsInProcessedMap;
         numRecsInProcessedMap = aggregate(processedInputMap, processedInputMap, 0);
-        LOG.info("Aggregated " + processedTuples + " processed tuples to " + numRecsInProcessedMap + " tuples");
+        LOG.debug("Aggregated " + processedTuples + " processed tuples to " + numRecsInProcessedMap + " tuples");
     }
 
     private Tuple createValueTuple(Object key, List<Tuple> inpTuples) throws ExecException {
diff --git a/src/org/apache/pig/builtin/JsonMetadata.java b/src/org/apache/pig/builtin/JsonMetadata.java
index 08a6f5d..33de3c3 100644
--- a/src/org/apache/pig/builtin/JsonMetadata.java
+++ b/src/org/apache/pig/builtin/JsonMetadata.java
@@ -22,6 +22,9 @@ import java.io.IOException;
 import java.io.OutputStream;
 import java.util.HashSet;
 import java.util.Set;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.TimeUnit;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -51,6 +54,11 @@ import org.codehaus.jackson.map.JsonMappingException;
 import org.codehaus.jackson.map.ObjectMapper;
 import org.codehaus.jackson.map.util.LRUMap;
 
+import com.google.common.base.Optional;
+import com.google.common.base.Throwables;
+import com.google.common.cache.Cache;
+import com.google.common.cache.CacheBuilder;
+
 /**
  * Reads and Writes metadata using JSON in metafiles next to the data.
  *
@@ -71,6 +79,13 @@ public class JsonMetadata implements LoadMetadata, StoreMetadata {
 
     private transient LRUMap<ElementDescriptor, Boolean> lookupCache = new LRUMap<ElementDescriptor, Boolean>(100, 1000);
 
+    private static transient Cache<String, Optional<ResourceSchema>> SCHEMA_CACHE = CacheBuilder.newBuilder().
+            concurrencyLevel(3).
+            expireAfterAccess(1, TimeUnit.MINUTES).
+            initialCapacity(25).
+            maximumSize(100).
+            build();
+
     public JsonMetadata() {
         this(".pig_schema", ".pig_header", ".pig_stats");
     }
@@ -173,6 +188,20 @@ public class JsonMetadata implements LoadMetadata, StoreMetadata {
         return getSchema(location, job, false);
     }
 
+    public ResourceSchema getSchema(final String location, final Job job, final boolean isSchemaOn) throws IOException {
+        try {
+            return SCHEMA_CACHE.get(location, new Callable<Optional<ResourceSchema>>() {
+                @Override
+                public Optional<ResourceSchema> call() throws Exception {
+                    return Optional.fromNullable(
+                            loadSchema(location, job, isSchemaOn));
+                }}).orNull();
+        } catch (ExecutionException e) {
+            Throwables.propagateIfInstanceOf(e.getCause(), IOException.class);
+            throw Throwables.propagate(e.getCause());
+        }
+    }
+
     /**
      * Read the schema from json metadata file
      * If isSchemaOn parameter is false, the errors are suppressed and logged
@@ -182,7 +211,7 @@ public class JsonMetadata implements LoadMetadata, StoreMetadata {
      * @return schema
      * @throws IOException
      */
-    public ResourceSchema getSchema(String location, Job job, boolean isSchemaOn) throws IOException {
+    public ResourceSchema loadSchema(String location, Job job, boolean isSchemaOn) throws IOException {
         Configuration conf = job.getConfiguration();
         Set<ElementDescriptor> schemaFileSet = null;
         try {
diff --git a/test/org/apache/pig/test/TestPOPartialAggPlan.java b/test/org/apache/pig/test/TestPOPartialAggPlan.java
index 95973a9..82ab155 100644
--- a/test/org/apache/pig/test/TestPOPartialAggPlan.java
+++ b/test/org/apache/pig/test/TestPOPartialAggPlan.java
@@ -17,9 +17,7 @@
  */
 package org.apache.pig.test;
 
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertNotNull;
-import static org.junit.Assert.assertNull;
+import static org.junit.Assert.*;
 
 import java.util.Iterator;
 
@@ -83,6 +81,41 @@ public class TestPOPartialAggPlan  {
 
     }
 
+    @Test
+    public void testMapAggPropTrueNoNoCombinerProp() throws Exception{
+        //test with pig.exec.mapPartAgg to true
+        String query = getGByQuery();
+        pc.getProperties().setProperty(PigConfiguration.PROP_EXEC_MAP_PARTAGG, "true");
+        MROperPlan mrp = Util.buildMRPlan(query, pc);
+        assertEquals(mrp.size(), 1);
+        assertFalse(mrp.getRoots().get(0).combinePlan.isEmpty());
+
+        assertNotNull("POPartialAgg should be present",findPOPartialAgg(mrp));
+    }
+    @Test
+    public void testMapAggPropTrueNoCombinerPropTrue() throws Exception{
+        //test with pig.exec.mapPartAgg to true
+        String query = getGByQuery();
+        pc.getProperties().setProperty(PigConfiguration.PROP_EXEC_MAP_PARTAGG, "true");
+        pc.getProperties().setProperty(PigConfiguration.PROP_NO_COMBINER, "true");
+        MROperPlan mrp = Util.buildMRPlan(query, pc);
+        assertEquals(mrp.size(), 1);
+        assertTrue(mrp.getRoots().get(0).combinePlan.isEmpty());
+
+        assertNotNull("POPartialAgg should be present",findPOPartialAgg(mrp));
+    }
+    @Test
+    public void testMapAggPropTrueNoCombinerFalse() throws Exception{
+        //test with pig.exec.mapPartAgg to true
+        String query = getGByQuery();
+        pc.getProperties().setProperty(PigConfiguration.PROP_EXEC_MAP_PARTAGG, "true");
+        pc.getProperties().setProperty(PigConfiguration.PROP_NO_COMBINER, "false");
+        MROperPlan mrp = Util.buildMRPlan(query, pc);
+        assertEquals(mrp.size(), 1);
+        assertFalse(mrp.getRoots().get(0).combinePlan.isEmpty());
+
+        assertNotNull("POPartialAgg should be present",findPOPartialAgg(mrp));
+    }
 
     private Object findPOPartialAgg(MROperPlan mrp) {
         PhysicalPlan mapPlan = mrp.getRoots().get(0).mapPlan;
