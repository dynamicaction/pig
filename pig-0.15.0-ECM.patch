#
# Patch with changes to Pig 0.15.0 to fix the following:
# - Excessive logging in POPartialAgg 
# - Out of memory in combiner with large data sets (applied patch PIG-2829.separate.options.patch to allow combiner to be disabled) [Opened PIG-4002]
# - NOT ADDED TO 0.13.0 PENDING VERIFICATION OF NEED: UDFContext not read in PigCombiner (custom change to PigCombiner) [Need to determine if still needed]
# - Schema cache in JsonMetadata. Speeds up parsing immensely. This works for us because files don't change during a run, but probably not universally applicable, so no patch submitted.
# - Fix for overactive spills with MultiQueryPackager (customChange to MultiQueryOptimizer - failed to set numInputs on packager)
#
# Useful commands:
#     Build all: ant -Dhadoopversion=23
#     Create eclipse files: ant eclipse-files
#     Create source jar: ant source-jar
#     Install locally: mvn install:install-file -Dfile=build/pig-0.15.0-SNAPSHOT.jar -Dsources=build/pig-0.15.0-SNAPSHOT-sources.jar -DartifactId=pig-ecm -DgroupId=org.apache.pig -Dversion=0.15.0-SNAPSHOT -Dpackaging=jar -DpomFile=pom.xml
#     Deploy to repo:  mvn deploy:deploy-file -Dfile=build/pig-0.15.0-SNAPSHOT.jar -DartifactId=pig-ecm -DgroupId=org.apache.pig -Dversion=0.15.0-SNAPSHOT -Dpackaging=jar -DpomFile=pom.xml -Dsources=build/pig-0.15.0-SNAPSHOT-sources.jar -DrepositoryId=itrader-snapshots -Durl=http://build1.dev1.dynamicaction.com:8081/archiva/repository/snapshots/
#     
#
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
index 595e68c..36d1d91 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
@@ -53,6 +53,7 @@ import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
 import org.apache.pig.backend.hadoop.executionengine.JobCreationException;
 import org.apache.pig.backend.hadoop.executionengine.Launcher;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.LastInputStreamingOptimizer;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.CombinerPlanRemover;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.DotMRPrinter;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.EndOfAllInputSetter;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MRIntermediateDataVisitor;
@@ -638,10 +639,18 @@ public class MapReduceLauncher extends Launcher{
                 pc.getProperties().getProperty(
                         "last.input.chunksize", JoinPackager.DEFAULT_CHUNK_SIZE);
 
-        String prop = pc.getProperties().getProperty(PigConfiguration.PIG_EXEC_NO_COMBINER);
-        if (!pc.inIllustrator && !("true".equals(prop)))  {
-            boolean doMapAgg =
-                    Boolean.valueOf(pc.getProperties().getProperty(PigConfiguration.PIG_EXEC_MAP_PARTAGG,"false"));
+        boolean doMapAgg =
+            Boolean.valueOf(pc.getProperties().getProperty(PigConfiguration.PROP_EXEC_MAP_PARTAGG,"false"));
+
+        boolean doCombiner = 
+            !Boolean.valueOf(pc.getProperties().getProperty(PigConfiguration.PROP_NO_COMBINER,
+                    // Default to no combiner when doMapAgg is true
+                    String.valueOf(doMapAgg))); 
+
+        // If doMapAgg is true, add the combiner even if it was disabled, since
+        // CombinerOptimizer actually adds the POPartialAgg plan. We'll remove 
+        // the combine plan later.
+        if (!pc.inIllustrator && (doCombiner || doMapAgg))  {
             CombinerOptimizer co = new CombinerOptimizer(plan, doMapAgg);
             co.visit();
             //display the warning message(s) from the CombinerOptimizer
@@ -660,7 +669,7 @@ public class MapReduceLauncher extends Launcher{
             la.adjust();
         }
         // Optimize to use secondary sort key if possible
-        prop = pc.getProperties().getProperty(PigConfiguration.PIG_EXEC_NO_SECONDARY_KEY);
+        String prop = pc.getProperties().getProperty(PigConfiguration.PIG_EXEC_NO_SECONDARY_KEY);
         if (!pc.inIllustrator && !("true".equals(prop)))  {
             SecondaryKeyOptimizerMR skOptimizer = new SecondaryKeyOptimizerMR(plan);
             skOptimizer.visit();
@@ -669,6 +678,15 @@ public class MapReduceLauncher extends Launcher{
         // optimize key - value handling in package
         POPackageAnnotator pkgAnnotator = new POPackageAnnotator(plan);
         pkgAnnotator.visit();
+        
+               
+        // now we can remove the combiner plan if we don't really need it.
+        // note the combiner plan is needed by POPackageAnnotator
+        if (!doCombiner){
+            CombinerPlanRemover remover = new CombinerPlanRemover(plan);
+            remover.visit();
+        }
+        
 
         // optimize joins
         LastInputStreamingOptimizer liso =
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MultiQueryOptimizer.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MultiQueryOptimizer.java
index 5331766..dc1dc9a 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MultiQueryOptimizer.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MultiQueryOptimizer.java
@@ -1194,6 +1194,7 @@ class MultiQueryOptimizer extends MROpPlanVisitor {
         MultiQueryPackager pkgr = new MultiQueryPackager();
         pkgr.setInCombiner(inCombiner);
         pkgr.setSameMapKeyType(sameMapKeyType);
+        pkgr.setNumInputs(pkg.getNumInps());
         pkg.setPkgr(pkgr);
         return pkg;
     }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/plans/CombinerPlanRemover.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/plans/CombinerPlanRemover.java
new file mode 100644
index 0000000..f2a9be0
--- /dev/null
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/plans/CombinerPlanRemover.java
@@ -0,0 +1,21 @@
+package org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans;
+
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
+import org.apache.pig.impl.plan.DepthFirstWalker;
+import org.apache.pig.impl.plan.VisitorException;
+
+public class CombinerPlanRemover extends MROpPlanVisitor {
+
+    public CombinerPlanRemover(MROperPlan plan) {
+        super(plan, new DepthFirstWalker<MapReduceOper, MROperPlan>(plan));
+    }
+    @Override
+    public void visitMROp(MapReduceOper mr) throws VisitorException {
+        
+
+        if(!mr.combinePlan.isEmpty()) {
+            mr.combinePlan = new PhysicalPlan();
+        }
+    }
+}
\ No newline at end of file
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPartialAgg.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPartialAgg.java
index 27551f2..0478d75 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPartialAgg.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPartialAgg.java
@@ -233,7 +233,7 @@ public class POPartialAgg extends PhysicalOperator implements Spillable, Groupin
                     if (parentPlan.endOfAllInput) {
                         // parent input is over. flush what we have.
                         inputsExhausted = true;
-                        LOG.info("Spilling last bits.");
+                        LOG.debug("Spilling last bits.");
                         startSpill(true);
                         continue;
                     } else {
@@ -267,7 +267,7 @@ public class POPartialAgg extends PhysicalOperator implements Spillable, Groupin
 
     private void estimateMemThresholds() {
         if (!mapAggDisabled()) {
-            LOG.info("Getting mem limits; considering " + ALL_POPARTS.size()
+            LOG.debug("Getting mem limits; considering " + ALL_POPARTS.size()
                     + " POPArtialAgg objects." + " with memory percentage "
                     + percentUsage);
             MemoryLimits memLimits = new MemoryLimits(ALL_POPARTS.size(), percentUsage);
@@ -283,10 +283,10 @@ public class POPartialAgg extends PhysicalOperator implements Spillable, Groupin
             }
             avgTupleSize = estTotalMem / estTuples;
             long totalTuples = memLimits.getCacheLimit();
-            LOG.info("Estimated total tuples to buffer, based on " + estTuples + " tuples that took up " + estTotalMem + " bytes: " + totalTuples);
+            LOG.debug("Estimated total tuples to buffer, based on " + estTuples + " tuples that took up " + estTotalMem + " bytes: " + totalTuples);
             firstTierThreshold = (int) (0.5 + totalTuples * (1f - (1f / sizeReduction)));
             secondTierThreshold = (int) (0.5 + totalTuples *  (1f / sizeReduction));
-            LOG.info("Setting thresholds. Primary: " + firstTierThreshold + ". Secondary: " + secondTierThreshold);
+            LOG.debug("Setting thresholds. Primary: " + firstTierThreshold + ". Secondary: " + secondTierThreshold);
             // The second tier should at least allow one tuple before it tries to aggregate.
             // This code retains the total number of tuples in the buffer while guaranteeing
             // the second tier has at least one tuple.
@@ -303,12 +303,12 @@ public class POPartialAgg extends PhysicalOperator implements Spillable, Groupin
             int numBeforeReduction = numRecsInProcessedMap + numRecsInRawMap;
             aggregateBothLevels(false, false);
             int numAfterReduction = numRecsInProcessedMap + numRecsInRawMap;
-            LOG.info("After reduction, processed map: " + numRecsInProcessedMap + "; raw map: " + numRecsInRawMap);
-            LOG.info("Observed reduction factor: from " + numBeforeReduction +
+            LOG.debug("After reduction, processed map: " + numRecsInProcessedMap + "; raw map: " + numRecsInRawMap);
+            LOG.debug("Observed reduction factor: from " + numBeforeReduction +
                     " to " + numAfterReduction +
                     " => " + numBeforeReduction / numAfterReduction + ".");
             if ( numBeforeReduction / numAfterReduction < minOutputReduction) {
-                LOG.info("Disabling in-memory aggregation, since observed reduction is less than " + minOutputReduction);
+                LOG.debug("Disabling in-memory aggregation, since observed reduction is less than " + minOutputReduction);
                 disableMapAgg();
             }
             sizeReduction = numBeforeReduction / numAfterReduction;
@@ -365,7 +365,7 @@ public class POPartialAgg extends PhysicalOperator implements Spillable, Groupin
         // If spillingIterator is null, we are already spilling and don't need to set up.
         if (spillingIterator != null) return;
 
-        LOG.info("Starting spill.");
+        LOG.debug("Starting spill.");
         if (aggregate) {
             aggregateBothLevels(false, true);
         }
@@ -377,7 +377,7 @@ public class POPartialAgg extends PhysicalOperator implements Spillable, Groupin
         // if no more to spill, return EOP_RESULT.
         if (processedInputMap.isEmpty()) {
             spillingIterator = null;
-            LOG.info("In spillResults(), processed map is empty -- done spilling.");
+            LOG.debug("In spillResults(), processed map is empty -- done spilling.");
             return EOP_RESULT;
         } else {
             Map.Entry<Object, List<Tuple>> entry = spillingIterator.next();
@@ -437,7 +437,7 @@ public class POPartialAgg extends PhysicalOperator implements Spillable, Groupin
         int processedTuples = numRecsInProcessedMap;
         numRecsInProcessedMap = aggregate(rawInputMap, processedInputMap, numRecsInProcessedMap);
         numRecsInRawMap = 0;
-        LOG.info("Aggregated " + rawTuples+ " raw tuples."
+        LOG.debug("Aggregated " + rawTuples+ " raw tuples."
                 + " Processed tuples before aggregation = " + processedTuples
                 + ", after aggregation = " + numRecsInProcessedMap);
     }
diff --git a/src/org/apache/pig/builtin/JsonMetadata.java b/src/org/apache/pig/builtin/JsonMetadata.java
index 08a6f5d..1342d76 100644
--- a/src/org/apache/pig/builtin/JsonMetadata.java
+++ b/src/org/apache/pig/builtin/JsonMetadata.java
@@ -22,6 +22,9 @@ import java.io.IOException;
 import java.io.OutputStream;
 import java.util.HashSet;
 import java.util.Set;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.TimeUnit;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -51,6 +54,11 @@ import org.codehaus.jackson.map.JsonMappingException;
 import org.codehaus.jackson.map.ObjectMapper;
 import org.codehaus.jackson.map.util.LRUMap;
 
+import com.google.common.base.Optional;
+import com.google.common.base.Throwables;
+import com.google.common.cache.Cache;
+import com.google.common.cache.CacheBuilder;
+
 /**
  * Reads and Writes metadata using JSON in metafiles next to the data.
  *
@@ -70,6 +78,13 @@ public class JsonMetadata implements LoadMetadata, StoreMetadata {
     private byte recordDel;
 
     private transient LRUMap<ElementDescriptor, Boolean> lookupCache = new LRUMap<ElementDescriptor, Boolean>(100, 1000);
+    
+    private static transient Cache<String, Optional<ResourceSchema>> SCHEMA_CACHE = CacheBuilder.newBuilder().
+            concurrencyLevel(3).
+            expireAfterAccess(1, TimeUnit.MINUTES).
+            initialCapacity(25).
+            maximumSize(100).
+            build();
 
     public JsonMetadata() {
         this(".pig_schema", ".pig_header", ".pig_stats");
@@ -172,6 +187,20 @@ public class JsonMetadata implements LoadMetadata, StoreMetadata {
     public ResourceSchema getSchema(String location, Job job) throws IOException {
         return getSchema(location, job, false);
     }
+    
+    public ResourceSchema getSchema(final String location, final Job job, final boolean isSchemaOn) throws IOException {
+        try {
+            return SCHEMA_CACHE.get(location, new Callable<Optional<ResourceSchema>>() {
+                @Override
+                public Optional<ResourceSchema> call() throws Exception {
+                    return Optional.fromNullable(
+                            loadSchema(location, job, isSchemaOn));
+                }}).orNull();
+        } catch (ExecutionException e) {
+            Throwables.propagateIfInstanceOf(e.getCause(), IOException.class);
+            throw Throwables.propagate(e.getCause());
+        }
+    }
 
     /**
      * Read the schema from json metadata file
@@ -182,7 +211,7 @@ public class JsonMetadata implements LoadMetadata, StoreMetadata {
      * @return schema
      * @throws IOException
      */
-    public ResourceSchema getSchema(String location, Job job, boolean isSchemaOn) throws IOException {
+    private ResourceSchema loadSchema(String location, Job job, boolean isSchemaOn) throws IOException {
         Configuration conf = job.getConfiguration();
         Set<ElementDescriptor> schemaFileSet = null;
         try {
diff --git a/test/org/apache/pig/test/TestPOPartialAggPlan.java b/test/org/apache/pig/test/TestPOPartialAggPlan.java
index 95973a9..6bbf88a 100644
--- a/test/org/apache/pig/test/TestPOPartialAggPlan.java
+++ b/test/org/apache/pig/test/TestPOPartialAggPlan.java
@@ -17,9 +17,7 @@
  */
 package org.apache.pig.test;
 
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertNotNull;
-import static org.junit.Assert.assertNull;
+import static org.junit.Assert.*;
 
 import java.util.Iterator;
 
@@ -80,9 +78,45 @@ public class TestPOPartialAggPlan  {
         assertEquals(mrp.size(), 1);
 
         assertNotNull("POPartialAgg should be present",findPOPartialAgg(mrp));
-
     }
-
+    
+    @Test
+    public void testMapAggPropTrueNoNoCombinerProp() throws Exception{
+        //test with pig.exec.mapPartAgg to true
+        String query = getGByQuery();
+        pc.getProperties().setProperty(PigConfiguration.PROP_EXEC_MAP_PARTAGG, "true");
+        MROperPlan mrp = Util.buildMRPlan(query, pc);
+        assertEquals(mrp.size(), 1);
+        assertFalse(mrp.getRoots().get(0).combinePlan.isEmpty());
+        
+        assertNotNull("POPartialAgg should be present",findPOPartialAgg(mrp));
+    }
+    
+    @Test
+    public void testMapAggPropTrueNoCombinerPropTrue() throws Exception{
+        //test with pig.exec.mapPartAgg to true
+        String query = getGByQuery();
+        pc.getProperties().setProperty(PigConfiguration.PROP_EXEC_MAP_PARTAGG, "true");
+        pc.getProperties().setProperty(PigConfiguration.PROP_NO_COMBINER, "true");
+        MROperPlan mrp = Util.buildMRPlan(query, pc);
+        assertEquals(mrp.size(), 1);
+        assertTrue(mrp.getRoots().get(0).combinePlan.isEmpty());
+        
+        assertNotNull("POPartialAgg should be present",findPOPartialAgg(mrp));
+    }
+    
+    @Test
+    public void testMapAggPropTrueNoCombinerFalse() throws Exception{
+        //test with pig.exec.mapPartAgg to true
+        String query = getGByQuery();
+        pc.getProperties().setProperty(PigConfiguration.PROP_EXEC_MAP_PARTAGG, "true");
+        pc.getProperties().setProperty(PigConfiguration.PROP_NO_COMBINER, "false");
+        MROperPlan mrp = Util.buildMRPlan(query, pc);
+        assertEquals(mrp.size(), 1);
+        assertFalse(mrp.getRoots().get(0).combinePlan.isEmpty());
+        
+        assertNotNull("POPartialAgg should be present",findPOPartialAgg(mrp));
+    }
 
     private Object findPOPartialAgg(MROperPlan mrp) {
         PhysicalPlan mapPlan = mrp.getRoots().get(0).mapPlan;
-- 
2.1.4

